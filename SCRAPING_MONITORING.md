# üï∑Ô∏è Guia Completo de Monitoramento do Web Scraping - JusCash DJE

## üéØ Como Analisar se o Web Scraping est√° Funcionando Corretamente

### üìä **1. Flower - Monitor Principal para Scraping**

**URL**: https://flower.juscash.app  
**Login**: admin / juscash2024

**O que verificar no Flower:**

#### üîç **Aba "Tasks"**
- Procure por tarefas com nomes:
  - `extract_publicacoes_task`
  - `extract_daily_publicacoes`
  - `extract_full_period_publicacoes`
  - `extract_custom_period_publicacoes`

#### üìà **Estados das Tarefas de Scraping**
- **SUCCESS** ‚úÖ: Scraping conclu√≠do com sucesso
- **PROGRESS** üîÑ: Scraping em execu√ß√£o
- **FAILURE** ‚ùå: Erro no scraping
- **PENDING** ‚è≥: Aguardando execu√ß√£o

#### üïê **Tempo de Execu√ß√£o**
- Scraping normal: 2-10 minutos
- Scraping de per√≠odo longo: 10-60 minutos
- Se passar de 60 minutos: poss√≠vel problema

### üìã **2. Logs Detalhados do Scraping**

```bash
# Logs espec√≠ficos do worker (onde roda o scraping)
ssh root@77.37.68.178 "cd /var/www/juscash && docker logs juscash_worker_prod --tail 100 | grep -i 'scraping\|publicacao\|extract\|selenium\|chrome'"

# Logs em tempo real
ssh root@77.37.68.178 "cd /var/www/juscash && docker logs juscash_worker_prod -f | grep -E '(scraping|publicacao|extract|selenium|chrome|DJE)'"

# Filtrar apenas sucessos
ssh root@77.37.68.178 "cd /var/www/juscash && docker logs juscash_worker_prod --tail 50 | grep -i 'conclu√≠da\|success\|extra√≠das'"

# Filtrar apenas erros
ssh root@77.37.68.178 "cd /var/www/juscash && docker logs juscash_worker_prod --tail 50 | grep -i 'error\|erro\|failed\|falha'"
```

**Logs de Sucesso Esperados:**
```
‚úÖ Driver inicializado com webdriver-manager
Iniciando extra√ß√£o de 01/12/2024 a 01/12/2024
Processando p√°gina de resultados 1...
Raspagem di√°ria conclu√≠da: 5 publica√ß√µes extra√≠das do dia 2024-12-01
‚úÖ Driver do Selenium finalizado com sucesso
```

**Logs de Erro a Investigar:**
```
‚ùå Falha cr√≠tica ao inicializar Chrome ap√≥s 3 tentativas
Driver n√£o est√° operacional. Abortando extra√ß√£o.
Erro fatal durante a extra√ß√£o: ...
connection to server at "db" failed
```

### üß™ **3. Testes Manuais via API**

#### **Teste R√°pido de Scraping**
```bash
# Testar scraping de 1 dia (ontem)
curl -X POST 'https://cron.juscash.app/api/scraping/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "data_inicio": "2024-12-25T00:00:00",
    "data_fim": "2024-12-25T23:59:59"
  }'
```

**Resposta de Sucesso:**
```json
{
  "task_id": "a6102a28-85b0-4e22-9fdd-f00da25918d3",
  "status": "Em processamento (async)",
  "message": "Extra√ß√£o de publica√ß√µes iniciada em background via Celery"
}
```

#### **Verificar Status da Tarefa**
```bash
# Usar o task_id da resposta anterior
curl -X GET 'https://cron.juscash.app/api/cron/tasks/a6102a28-85b0-4e22-9fdd-f00da25918d3'
```

**Resposta de Sucesso:**
```json
{
  "state": "SUCCESS",
  "result": {
    "total_extraidas": 8,
    "data_inicio": "2024-12-25T00:00:00",
    "data_fim": "2024-12-25T23:59:59",
    "status": "concluido"
  }
}
```

### üìä **4. Verifica√ß√£o do Banco de Dados**

#### **Contar Publica√ß√µes Extra√≠das**
```bash
# Verificar quantas publica√ß√µes foram salvas
curl -X GET 'https://cron.juscash.app/api/publicacoes/?limit=1000' | python3 -c "
import sys, json
data = json.load(sys.stdin)
print(f'Total de publica√ß√µes: {len(data)}')
print('√öltimas 5 publica√ß√µes:')
for pub in data[:5]:
    print(f'- {pub[\"numero_processo\"]} - {pub[\"data_disponibilizacao\"]}')
"
```

#### **Verificar Publica√ß√µes por Data**
```bash
# Publica√ß√µes de hoje
curl -X GET 'https://cron.juscash.app/api/publicacoes/?search=2024-12-26'

# Publica√ß√µes recentes
curl -X GET 'https://cron.juscash.app/api/publicacoes/?limit=10'
```

### üîç **5. An√°lise de Qualidade dos Dados**

#### **Script Python para An√°lise Detalhada**
```python
import requests
import json
from datetime import datetime, timedelta

API_BASE = "https://cron.juscash.app/api"

def analisar_qualidade_scraping():
    """Analisa a qualidade dos dados extra√≠dos"""
    
    # Buscar publica√ß√µes recentes
    response = requests.get(f"{API_BASE}/publicacoes/?limit=100")
    publicacoes = response.json()
    
    print(f"üìä AN√ÅLISE DE QUALIDADE DO SCRAPING")
    print(f"=" * 50)
    print(f"Total de publica√ß√µes: {len(publicacoes)}")
    
    if not publicacoes:
        print("‚ùå Nenhuma publica√ß√£o encontrada!")
        return
    
    # An√°lise de campos obrigat√≥rios
    campos_vazios = {
        'numero_processo': 0,
        'autores': 0,
        'advogados': 0,
        'conteudo_completo': 0
    }
    
    publicacoes_com_valores = 0
    datas_extraidas = set()
    
    for pub in publicacoes:
        # Verificar campos vazios
        for campo in campos_vazios:
            if not pub.get(campo) or pub[campo].strip() == '':
                campos_vazios[campo] += 1
        
        # Contar publica√ß√µes com valores monet√°rios
        if any([
            pub.get('valor_principal_bruto'),
            pub.get('valor_principal_liquido'),
            pub.get('valor_juros_moratorios'),
            pub.get('honorarios_advocaticios')
        ]):
            publicacoes_com_valores += 1
        
        # Coletar datas
        if pub.get('data_disponibilizacao'):
            data = pub['data_disponibilizacao'][:10]  # YYYY-MM-DD
            datas_extraidas.add(data)
    
    print(f"\nüîç QUALIDADE DOS DADOS:")
    print(f"Publica√ß√µes com valores monet√°rios: {publicacoes_com_valores} ({publicacoes_com_valores/len(publicacoes)*100:.1f}%)")
    print(f"Datas diferentes encontradas: {len(datas_extraidas)}")
    print(f"Per√≠odo coberto: {min(datas_extraidas) if datas_extraidas else 'N/A'} a {max(datas_extraidas) if datas_extraidas else 'N/A'}")
    
    print(f"\n‚ùå CAMPOS VAZIOS:")
    for campo, count in campos_vazios.items():
        porcentagem = count/len(publicacoes)*100
        status = "‚úÖ" if porcentagem < 10 else "‚ö†Ô∏è" if porcentagem < 50 else "‚ùå"
        print(f"{status} {campo}: {count} vazios ({porcentagem:.1f}%)")
    
    # √öltimas extra√ß√µes
    print(f"\nüìÖ √öLTIMAS EXTRA√á√ïES:")
    for data in sorted(datas_extraidas, reverse=True)[:5]:
        count = sum(1 for pub in publicacoes if pub.get('data_disponibilizacao', '').startswith(data))
        print(f"- {data}: {count} publica√ß√µes")

# Executar an√°lise
analisar_qualidade_scraping()
```

### üö® **6. Indicadores de Problemas no Scraping**

#### **Problemas Cr√≠ticos:**
- ‚ùå **0 publica√ß√µes extra√≠das**: Scraper n√£o conseguiu acessar o site
- ‚ùå **Timeout constante**: Problemas de conectividade ou site lento
- ‚ùå **Erro de Chrome/Selenium**: Driver n√£o inicializa
- ‚ùå **Erro de banco**: N√£o consegue salvar os dados

#### **Problemas de Qualidade:**
- ‚ö†Ô∏è **Muitos campos vazios**: Parsing dos dados falhou
- ‚ö†Ô∏è **N√∫meros de processo inv√°lidos**: Regex n√£o funcionou
- ‚ö†Ô∏è **Datas inconsistentes**: Problema na extra√ß√£o de datas
- ‚ö†Ô∏è **Conte√∫do duplicado**: L√≥gica de deduplica√ß√£o falhou

#### **Sinais de Sucesso:**
- ‚úÖ **Publica√ß√µes extra√≠das > 0**: Scraping funcionando
- ‚úÖ **Campos preenchidos > 80%**: Parsing funcionando bem
- ‚úÖ **Valores monet√°rios extra√≠dos**: Regex de valores funcionando
- ‚úÖ **Tempo de execu√ß√£o consistente**: Performance est√°vel

### üõ†Ô∏è **7. Comandos de Diagn√≥stico Espec√≠ficos**

#### **Verificar Chrome/Selenium**
```bash
# Testar se Chrome est√° funcionando no container
ssh root@77.37.68.178 "cd /var/www/juscash && docker exec juscash_worker_prod google-chrome --version"

# Testar Selenium
ssh root@77.37.68.178 "cd /var/www/juscash && docker exec juscash_worker_prod python -c 'from selenium import webdriver; print(\"Selenium OK\")'"
```

#### **Testar Conectividade com DJE**
```bash
# Testar acesso ao site do DJE
ssh root@77.37.68.178 "cd /var/www/juscash && docker exec juscash_worker_prod curl -I https://dje.tjsp.jus.br/cdje/index.do"
```

#### **Verificar Recursos do Container**
```bash
# Mem√≥ria e CPU do worker
ssh root@77.37.68.178 "cd /var/www/juscash && docker stats juscash_worker_prod --no-stream"
```

### üìà **8. M√©tricas de Performance**

#### **Benchmarks Esperados:**
- **Velocidade**: 10-50 publica√ß√µes por minuto
- **Uso de mem√≥ria**: 200-800 MB durante scraping
- **Uso de CPU**: 20-60% durante scraping
- **Taxa de sucesso**: > 95% das execu√ß√µes

#### **Script de Monitoramento Cont√≠nuo**
```python
import requests
import time
from datetime import datetime

def monitorar_scraping_continuo():
    """Monitora o scraping continuamente"""
    
    while True:
        try:
            # Executar scraping de teste (1 dia)
            ontem = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            
            response = requests.post('https://cron.juscash.app/api/scraping/extract', json={
                'data_inicio': f'{ontem}T00:00:00',
                'data_fim': f'{ontem}T23:59:59'
            })
            
            if response.status_code == 200:
                data = response.json()
                task_id = data.get('task_id')
                
                print(f"üöÄ [{datetime.now()}] Scraping iniciado: {task_id}")
                
                # Aguardar conclus√£o
                while True:
                    status_response = requests.get(f'https://cron.juscash.app/api/cron/tasks/{task_id}')
                    status_data = status_response.json()
                    
                    if status_data['state'] == 'SUCCESS':
                        result = status_data.get('result', {})
                        total = result.get('total_extraidas', 0)
                        print(f"‚úÖ [{datetime.now()}] Sucesso: {total} publica√ß√µes extra√≠das")
                        break
                    elif status_data['state'] == 'FAILURE':
                        print(f"‚ùå [{datetime.now()}] Falha no scraping")
                        break
                    
                    time.sleep(30)
            else:
                print(f"‚ùå [{datetime.now()}] Erro na API: {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå [{datetime.now()}] Erro: {e}")
        
        # Aguardar 1 hora antes do pr√≥ximo teste
        time.sleep(3600)

# Para executar monitoramento cont√≠nuo
# monitorar_scraping_continuo()
```

### üéØ **9. Checklist de Verifica√ß√£o R√°pida**

#### **‚úÖ Verifica√ß√£o Di√°ria (2 minutos)**
1. Acessar Flower: https://flower.juscash.app
2. Verificar se h√° tarefas `extract_` com status SUCCESS nas √∫ltimas 24h
3. Verificar logs: `docker logs juscash_worker_prod --tail 20`
4. Contar publica√ß√µes: `curl https://cron.juscash.app/api/publicacoes/?limit=1 | wc -l`

#### **üîç Verifica√ß√£o Semanal (10 minutos)**
1. Executar teste manual de scraping
2. Analisar qualidade dos dados com script Python
3. Verificar performance dos containers
4. Revisar logs de erro da semana

#### **üõ†Ô∏è Verifica√ß√£o Mensal (30 minutos)**
1. An√°lise completa de qualidade dos dados
2. Otimiza√ß√£o de performance se necess√°rio
3. Atualiza√ß√£o de depend√™ncias (Chrome, Selenium)
4. Backup e an√°lise de tend√™ncias

### üö® **10. Troubleshooting Comum**

#### **Problema: Chrome n√£o inicializa**
```bash
# Solu√ß√£o 1: Restart do worker
ssh root@77.37.68.178 "cd /var/www/juscash && docker-compose -f docker-compose.prod.yml restart worker"

# Solu√ß√£o 2: Rebuild do container
ssh root@77.37.68.178 "cd /var/www/juscash && docker-compose -f docker-compose.prod.yml up --build worker -d"
```

#### **Problema: Site DJE mudou layout**
```bash
# Verificar se o site est√° acess√≠vel
curl -I https://dje.tjsp.jus.br/cdje/index.do

# Logs espec√≠ficos de parsing
ssh root@77.37.68.178 "cd /var/www/juscash && docker logs juscash_worker_prod | grep -i 'parsing\|element\|selector'"
```

#### **Problema: Muitos dados vazios**
1. Verificar logs de parsing
2. Testar regex patterns manualmente
3. Verificar se estrutura HTML mudou
4. Atualizar seletores se necess√°rio

#### **Problema: Performance lenta**
```bash
# Verificar recursos
ssh root@77.37.68.178 "cd /var/www/juscash && docker stats --no-stream"

# Otimizar Chrome options se necess√°rio
# Reduzir per√≠odo de scraping
# Adicionar delays entre requisi√ß√µes
```

---

## üìä **Resumo Executivo**

### **üéØ Para verificar rapidamente se o scraping est√° funcionando:**

1. **üå∏ Flower**: https://flower.juscash.app ‚Üí Verificar tarefas SUCCESS
2. **üìã Logs**: `docker logs juscash_worker_prod --tail 20 | grep extract`
3. **üß™ Teste**: POST para `/api/scraping/extract` com data de ontem
4. **üìä Dados**: GET `/api/publicacoes/` para ver publica√ß√µes recentes

### **‚úÖ Indicadores de Sa√∫de:**
- Tarefas SUCCESS no Flower nas √∫ltimas 24h
- Logs sem erros cr√≠ticos
- Publica√ß√µes novas sendo salvas
- Tempo de execu√ß√£o < 30 minutos por dia

### **üö® Alertas Cr√≠ticos:**
- 0 publica√ß√µes extra√≠das por > 2 dias
- Erros de Chrome/Selenium constantes
- Timeout em > 50% das execu√ß√µes
- Campos vazios em > 80% dos dados